# Awesome-Large-Language-Model-Research-in-Management-Science
Collection of papers about management science with LLMs.

## ABS 3以上期刊文章
|序号|题目|期刊|网址|简介|
|-|-|-|-|-|
| 1 | **FinBERT: A Large Language Model for Extracting Information from Financial Text** | **Contemporary Accounting Research（ABS 4）** | <a href="https://onlinelibrary.wiley.com/doi/10.1111/1911-3846.12832">Link</a> | 英文语料库上预训练的BERT |
| 2 | GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice | Financial Research Letters（ABS 2） | <a href="https://www.sciencedirect.com/science/article/pii/S1544612323007055">Link</a> |
| 3 | ChatGPT for (Finance) research: The Bananarama Conjecture | Financial Research Letters（ABS 2） | <a href="https://www.sciencedirect.com/science/article/pii/S1544612323000363">Link</a> |  |
| 4 | **Generative AI for Economic Research: Use Cases and Implications for Economists** | **Journal of Economic Literature（ABS 4）** | <a href="https://www.dropbox.com/scl/fi/kk6duothtufsa8dhs3yat/LLMs_final.pdf?rlkey=bfadp97ej13ruceggecfeeiyi&dl=0">Link</a> | LLM如何应用于经济学研究 |

## 综述类
|序号|题目|期刊|机构|网址|简介|
|-|-|-|-|-|-|
| 1 | **Large Language Models: A Survey** | arXiv | 人大 | <a href="https://github.com/RUCAIBox/LLMSurvey">Link</a> | **LLM综述** |
| 2 | **A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future** | arXiv | 哈工大 | <a href="https://arxiv.org/abs/2309.15402">Link</a> | **思维链综述** |
| 3 | A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions | arXiv |  | <a href="https://arxiv.org/abs/2311.05232">Link</a> | 大模型幻觉综述 |
| 4 | Evaluating Large Language Models: A Comprehensive Survey | arXiv | 天大 | <a href="https://arxiv.org/abs/2310.19736">Link</a> | 大模型评价方法综述 |
| 5 | Challenges and Applications of Large Language Models | arXiv |  | <a href="https://arxiv.org/abs/2307.10169">Link</a> | 大模型的应用和挑战 |
| 6 | **Explainability for Large Language Models: A Survey** | arXiv | 新泽西理工学院、约翰霍普金斯大学等 | <a href="https://arxiv.org/abs/2309.01029">Link</a> | 大模型可解释性综述 |
| 7 | Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents | arXiv |  | <a href="https://arxiv.org/abs/2311.11797">Link</a> | 思维链与AI Agent综述 |
| 8 | The Rise and Potential of Large Language Model Based Agents: A Survey | arXiv | 复旦NLP | <a href="https://arxiv.org/abs/2309.07864">Link</a> | 大模型Agent综述 |
| 9 | A Survey of Reasoning with Foundation Models | arXiv | 港大、华为Nuah等 | <a href="https://arxiv.org/abs/2312.11562">Link</a> | 大模型推理综述（<a href="https://mp.weixin.qq.com/s/HK29hRpMiblDntA6MB72Pw">中文推荐</a>） |
| 10 | **A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications** | arXiv | 印度理工学院、斯坦福、Amazon | <a href="https://arxiv.org/abs/2402.07927">Link</a> | Prompt Engineering综述（<a href="https://mp.weixin.qq.com/s/va1Ua3koedNWKkln4v6Vvg">中文推荐</a>） |

## 技术类
|序号|题目|期刊|机构|网址|简介|
|-|-|-|-|-|-|
| 1 | **Attention Is All You Need** | arXiv |  | <a href="https://arxiv.org/abs/1706.03762">Link</a> | **Transformer** |
| 2 | Improving Language Understanding by Generative Pre-Training | arXiv |  | <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Link</a> | GPT-1 |
| 3 | **A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future** | arXiv |  | <a href="https://arxiv.org/abs/1810.04805">Link</a> | **BERT** |
| 4 | Language Models are Unsupervised Multitask Learners | arXiv |  | <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Link</a> | GPT-2 |
| 5 | Language Models are Few-Shot Learners | arXiv |  | <a href="https://arxiv.org/abs/2005.14165">Link</a> | GPT-3 |
| 6 | **Distilling the Knowledge in a Neural Network** | arXiv |  | <a href="https://arxiv.org/abs/1503.02531">Link</a> | **知识蒸馏的开山之作** |

## 领域大模型构建
|序号|题目|期刊|机构|网址|简介|
|-|-|-|-|-|-|
| 1 | FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models | arXiv | 港大 & 达摩院 | <a href="https://arxiv.org/abs/2308.00065">Link</a> | 金融风险大模型FinPT |
| 2 | BloombergGPT: A Large Language Model for Finance | arXiv | 彭博社 | <a href="https://arxiv.org/abs/2303.17564">Link</a> | 金融大模型BloombergGPT |
| 3 | XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters | arXiv | 度小满 | <a href="https://arxiv.org/abs/2305.12002">Link</a> | 金融大模型“轩辕” |
| 4 | ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases | arXiv | 北大 | <a href="http://arxiv.org/abs/2306.16092">Link</a> | 法律大模型ChatLaw |
| 5 | Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models | arXiv | 哥大 | <a href="http://arxiv.org/abs/2306.12659">Link</a> | 金融大模型Instruct-FinGPT |
| 6 | FinGPT: Open-Source Financial Large Language Models | arXiv | 哥大 | <a href="https://arxiv.org/abs/2306.06031">Link</a> | 金融大模型FinGPT |
| 7 | FinGPT: Democratizing Internet-scale Data for Financial Large Language Models | arXiv | 哥大 | <a href="https://arxiv.org/abs/2307.10485">Link</a> | 金融大模型FinGPT |
| 8 | FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis | arXiv | 哈工深 | <a href="https://arxiv.org/abs/2308.01430">Link</a> | 金融大模型FinVisGPT（多模态） |
| 9 | CFGPT: Chinese Financial Assistant with Large Language Model | arXiv | 同济 & 上海AI实验室 | <a href="https://arxiv.org/abs/2309.10654">Link</a> | 金融大模型CFGPT |
| 10 | DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning | arXiv | 复旦大学 | <a href="https://arxiv.org/abs/2310.15205">Link</a> | 金融资讯大模型DISC-FinLLM |
| 11 | PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance | arXiv | 武汉大学、中山大学等 | <a href="https://arxiv.org/abs/2306.05443">Link</a> | 金融大模型PIXIU |
| 12 | Data-Centric Financial Large Language Models | arXiv | 阿里巴巴、弗吉尼亚大学 | <a href="http://arxiv.org/abs/2310.17784">Link</a> | 金融大模型FLLM |
| 13 | BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark | arXiv | 复旦等 | <a href="http://arxiv.org/abs/2302.09432">Link</a> | 金融大模型Fin-T5 |

## 思维链与大模型推理
|序号|题目|期刊|机构|网址|简介|
|-|-|-|-|-|-|
| 1 | **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** | avXiv |  | <a href="https://arxiv.org/abs/2201.11903">Link</a> | CoT开山之作 |
| 2 | **Large Language Models are Zero-Shot Reasoners** | avXiv |  | <a href="http://arxiv.org/abs/2205.11916">Link</a> | Zero-shot-CoT |
| 3 | **Self-Consistency Improves Chain of Thought Reasoning in Language Models** | avXiv |  | <a href="https://arxiv.org/abs/2203.11171">Link</a> | CoT的早期发展：自洽性 |
| 4 | **Least-to-Most Prompting Enables Complex Reasoning in Large Language Models** | avXiv |  | <a href="https://arxiv.org/abs/2205.10625">Link</a> | CoT的又一发展（复杂任务分解） |
| 5 | **Large Language Models Are Reasoning Teachers** | avXiv |  | <a href="http://arxiv.org/abs/2212.10071">Link</a> | 利用大模型的CoT能力帮助小模型进行推理 |
| 6 | **Symbolic Chain-of-Thought Distillation: Small Models Can Also "Think" Step-by-Step** | avXiv |  | <a href="https://arxiv.org/abs/2306.14050">Link</a> | 利用大模型的CoT能力帮助小模型进行推理2号 |
| 7 | **Large Language Models Can Self-Improve** | avXiv |  | <a href="https://arxiv.org/abs/2210.11610">Link</a> | 自我改进 |
| 8 | Self-Refine: Iterative Refinement with Self-Feedback | avXiv |  | <a href="https://arxiv.org/abs/2303.17651">Link</a> | 自我反思 |
| 9 | Reflexion: Language Agents with Verbal Reinforcement Learning | avXiv |  | <a href="https://arxiv.org/abs/2303.11366">Link</a> | 自我反思2 |
| 10 | **Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes** | avXiv |  | <a href="https://arxiv.org/abs/2305.02301">Link</a> | 服了，怎么跟我这个RAL这么类似 |
| 11 | **Self-explaining AI as an alternative to interpretable AI** | avXiv |  | <a href="https://arxiv.org/abs/2002.05149">Link</a> | 自我解释 |
| 12 | **Beyond Classification: Financial Reasoning in State-of-the-Art Language Models** | avXiv |  | <a href="https://arxiv.org/abs/2305.01505">Link</a> | 金融推理问题数据集构建 |
| 13 | **Learning From Mistakes Makes LLM Better Reasoner** | avXiv |  | <a href="https://arxiv.org/abs/2310.20689">Link</a> | 让大模型在错题中学习 |
| 14 | Self-Discover: Large Language Models Self-Compose Reasoning Structures | avXiv | 南加州大学、谷歌DeepMind | <a href="https://arxiv.org/abs/2402.03620">Link</a> | **让大模型针对不同问题，提出特定的推理结构（<a href="https://mp.weixin.qq.com/s/HUW8MX2GhsdE3qFBvb1-Hg">中文解读</a>）** |

## AI Agents
|序号|题目|期刊|机构|网址|简介|
|-|-|-|-|-|-|
| 1 | Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus? | avXiv |  | <a href="http://arxiv.org/abs/2301.07543">Link</a> | ABM，模拟人类行为 |
| 2 | War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars | avXiv |  | <a href="https://arxiv.org/abs/2311.17227">Link</a> | 基于LLM的企业风险传染？基于大语言模型的多智能体仿真世界大战 |
| 3 | **Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives** | avXiv | 清华 | <a href="https://arxiv.org/abs/2312.11970">Link</a> | 对ABM和LLM的综述 |
| 4 | Generative Agents: Interactive Simulacra of Human Behavior | avXiv |  | <a href="https://arxiv.org/abs/2304.03442">Link</a> | 让大模型玩模拟人生 |

## Prompting Engineering
|序号|题目|期刊|机构|网址|简介|
|-|-|-|-|-|-|
| 1 | **Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine** | avXiv | 微软 | <a href="https://arxiv.org/abs/2311.16452">Link</a> | **医学领域使用GPT-4的prompt工程** |
| 2 | Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4 | avXiv |  | <a href="https://arxiv.org/abs/2312.16171">Link</a> |  |

## 大模型与主题模型
|序号|题目|期刊|机构|网址|简介|
|-|-|-|-|-|-|
| 1 | **TopicGPT: A Prompt-based Topic Modeling Framework** | avXiv |  | <a href="https://arxiv.org/abs/2311.01449">Link</a> | **<a href="https://mp.weixin.qq.com/s/vpnTiA5t3xis49NpZRNgpA">中文概述</a>** |
| 2 | **Prompting Large Language Models for Topic Modeling** | avXiv |  | <a href="http://arxiv.org/abs/2312.09693">Link</a> | **<a href="https://mp.weixin.qq.com/s/sbo9FmGgfyjy9QVbVeHn0Q">中文概述</a>** |

## Other
|序号|题目|期刊|机构|网址|简介|
|-|-|-|-|-|-|
| 1 | Can large language models provide useful feedback on research papers? A large-scale empirical analysis | avXiv | 斯坦福 | <a href="https://arxiv.org/abs/2310.01783">Link</a> | 大模型与审稿（<a href="https://mp.weixin.qq.com/s/9-F6cC9nB5kPO9rBBVmBuw">中文解读</a>） |
| 2 | When ChatGPT is gone: Creativity reverts and homogeneity persists | avXiv | 北京大学 | <a href="https://arxiv.org/abs/2401.06816">Link</a> | 大模型对长期创新能力的抑制（<a href="https://mp.weixin.qq.com/s/uQAubTdgaPmhbQbC6dhMUg">中文解读</a>） |
| 3 | When Advanced AI Isn't Enough: Human Factors as Drivers of Success in Generative AI-Human Collaborations | SSRN |  | <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4738829">Link</a> | ChatGPT时代的人机交互对绩效的影响研究（<a href="https://mp.weixin.qq.com/s/j_k-_DQqs-nDCS8XRJMQ5Q">中文简介</a>） |
